# Martian Helicopter Autonomous Navigation Project

<img src="readme_imgs/banner.jpeg" alt="Project screenshot" width="90%"/>

## Table of Contents

- [Overview](#overview)
- [Key Achievements](#key-achievements)
- [Hardware](#hardware)
- [Software](#software)
- [Mission Architecture](#mission-architecture)
- [Results](#results)

## Overview

This project demonstrates an autonomous navigation system for a Martian helicopter drone (inspired by NASA's Ingenuity) which is designed to explore and safely land in an unpredictable and hazardous environment. As a proof of concept, it simulates a drone that must successfully navigate to a predetermined target region, detect and avoid hazards in a dense boulder field, and execute a precision landing in a safe zone free from obstructions - all while handling persistent dust storms that obscure its vision. The system combines sensor fusion, state estimation, computer vision, AI, mission planning, and embedded robotics. 

The drone uses a downward-facing camera for optical flow velocity estimation using Shi-Tomasi feature detection and Lucas-Kanade tracking, enhanced by a preprocessing pipeline with optional Gaussian blur and histogram equalization (CLAHE) to improve feature robustness in low-contrast scenes. For hazard avoidance, there are two selectable modes: a baseline clearance based approach using the same features from the corner detector, or an AI variant with a YOLOv8 neural network trained on a custom dataset of labeled images that I generated by hand. The AI approach proves essential in dusty conditions where traditional CV methods can become unreliable. Sensor fusion is handled by integrating the camera, an IMU, and a LIDAR transceiver into an Extended Kalman Filter for accurate 3D pose and velocity estimation, and a state machine manages the mission phases with hysteresis for stable mode transitions. Annotated frames are recorded to HDF5 for detailed post-trial analysis.

**Demo Video:**

![Demo video](readme_imgs/recording_20260201_203336.gif)


> Note: This project uses a human-in-the-loop. The real-time algorithms produce on-screen flight directions that are carried out manually - this allows me to thoroughly validate the system without risking physical flight hardware.

## Key Achievements

- **Multi-sensor Fusion** — Implemented a 6-DoF Extended Kalman Filter combining visual velocity, LIDAR altitude, and IMU accelerometer/gyroscope data for robust 3D state estimation.
- **AI Hazard Detection** — Demonstrated significantly improved safe landing zone identification and mission success rate in dusty conditions when using a custom-trained YOLOv8 neural network hazard detector.
- **Martian Dust Simulation** — Built a realistic dust effects module with correlated Gaussian noise and drifting particle overlays to provide a challenging, low-visibility environment.
- **Intelligent Safe Zone Selection** — Engineered a hazard avoidance algorithm using distance transforms, hazard dilation, and proximity scoring to select the safest landing spot within a configurable search zone.
- **Real-time Visualizations** — Created a comprehensive real-time overlay with feature trails, navigation arrows, safe-zone heatmap, mode banners, altitude bars, and hover countdown, providing intuitive feedback during live hardware testing.

## Hardware

<img src="readme_imgs/hardware.jpeg" alt="Hardware" width="45%"/>

*The system runs on a Raspberry Pi 5 with several attached sensors to mimic a drone helicopter's payload.*

**Components:**
- **Raspberry Pi 5** — Core processing unit
- **ArduCam Camera Module 3** — Optical flow and visual input
- **IMU (ICM-20948)** — Acceleration and gyro data for orientation
- **LIDAR (VL53L1X)** — Precise altitude measurement

> Note: Also shown are a GPS receiver and an atmospheric pressure sensor. Although these were actively collecting data, they were ultimately unnecessary for this project. 

## Software

The software is written entirely in Python — it is modular, configurable, and optimized for real-time performance on embedded hardware.

**Directory structure:**

```text
src/autonomous_nav/
├── app.py                # Main application loop
├── config.py             # Centralized configuration
├── camera.py             # Camera handling & countdown
├── imu.py                # IMU driver & calibration
├── lidar.py              # LIDAR driver
├── preprocessor.py       # Image enhancement pipeline (CLAHE, blur)
├── feature_detector.py   # Shi-Tomasi corner detection
├── optical_flow.py       # Lucas-Kanade optical flow
├── state_estimator.py    # Extended Kalman Filter
├── hazard_avoidance.py   # Hazard detection (AI + classical)
├── dust.py               # Realistic Martian dust simulation
├── mission_manager.py    # Mission state machine
├── visualizer.py         # Real-time UI overlays & annotations
└── utils.py              # Helper functions
```

## Mission Architecture

The system follows a state machine that simulates a full Martian exploration and precision landing scenario:

1. `ASCENT` — Climb to cruising altitude.

2. `NAVIGATION` — Travel toward a predefined target location using optical flow velocity estimates and fused state estimation.

3. `SEARCHING` — Once inside the inner search radius, scan for safe landing zones using real-time hazard detection and select the safest candidate spot.

4. `LANDING_APPROACH` — Lock onto a stable safe site (median-filtered over multiple frames), navigate precisely toward it, and maintain clearance checks with hysteresis to avoid false triggers.

5. `DESCENT` & `HOVERING` — Descend to final altitude, transition to stable hover, and confirm landing readiness with a timed hover duration within position tolerance.

6. `LANDED_SAFE` / `NO_SAFE_ZONE` — Declare mission success on stable hover completion, or fall back to `NO_SAFE_ZONE` mode if no viable spot is found.

Transitions use hysteresis with consecutive-frame counters to prevent mode chattering even when detection becomes noisy.


## Results

The system was rigorously tested in live hardware runs on the Raspberry Pi 5, using real camera frames, IMU, and LIDAR data while manually following on-screen guidance. Through 100 trials simulating ascent, navigation, search, and landing under varying dust conditions and hazard detection modes, the full autonomy stack proved robust and reliable.

**Performance Highlights:**
- In a **clean environment**, both the classical (Shi-Tomasi) and AI (YOLOv8) hazard detectors reliably identified safe landing zones, with the system achieving stable hover and `LANDED_SAFE` mode in 90% of runs under these conditions.
- Under **heavy dust simulation**, classical feature-based detection frequently failed. Features were often clustered on dust artifacts instead of real hazards, and safe-zone selection became erratic, causing mission success rate to fall to 16%.
- Switching to the **custom YOLOv8 detector** restored reliable performance. Rock/hazard centroids were accurately identified even through dense dust overlays, safe-zone selection remained consistent, and mission success rate recovered to 88%. The AI approach was critical for maintaining mission viability in degraded visual environments, which are common on the Martian surface.


**Impact of Dust on Detection Methods:**

| Clean Image                  | Dust Simulation Overlay             |
|------------------------------|-----------------------------------|
| ![Clean](readme_imgs/no_dust.png)   | ![Dusty](readme_imgs/dusty.png)        |
| **Shi-Tomasi Corner Detector** | **AI YOLOv8 Rock Detector**    |
| ![Corners](readme_imgs/dusty_corners.png) | ![YOLO](readme_imgs/dusty_yolo.png)         |

*The side-by-side images above illustrate why the AI upgrade was necessary: in dusty frames, Shi-Tomasi corners latch onto drifting particles rather than stable surface features, while YOLO continues to detect actual rocks with high confidence.*

**Conclusion:**

This project brings together computer vision, deep learning, sensor fusion, state estimation, and embedded systems programming. It demonstrates the ability to build real-time data processing pipelines that operate reliably on resource-constrained hardware, while handling noisy and degraded sensor data in a challenging physical environment. The result is a system that showcases a well-rounded skill set in aerospace applications like planetary exploration, guidance and navigation, embedded AI applications, and autonomous robotics.